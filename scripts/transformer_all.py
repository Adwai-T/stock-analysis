import os
from pathlib import Path
from io import StringIO

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input,
    Dense,
    Dropout,
    LayerNormalization,
    MultiHeadAttention,
    GlobalAveragePooling1D,
)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# ---------- PATHS & CONFIG ----------

ROOT_DIR = Path(__file__).resolve().parent.parent
ALLDATA_DIR = ROOT_DIR / "data" / "allData"
MODEL_DIR = ROOT_DIR / "model"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

SEQ_LEN = 30          # days in input sequence
BATCH_SIZE = 64
EPOCHS = 30

MODEL_PATH = MODEL_DIR / "transformer_all_nifty200.h5"
SCALER_PATH = MODEL_DIR / "transformer_all_nifty200_scaler.npz"

REQUIRED_COLS = ["date", "open", "high", "low", "close", "volume"]


# ---------- FEATURE ENGINEERING ----------

def add_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Add engineered features to a stock OHLCV dataframe.
    Input df must contain: date, open, high, low, close, volume.
    Returns a new df with extra columns and NaNs dropped.
    """
    df = df.copy()
    df = df.sort_values("date").reset_index(drop=True)

    # ---- Returns ----
    df["pct_change"] = df["close"].pct_change()
    df["log_return"] = np.log(df["close"] / df["close"].shift(1))

    # ---- Moving Averages ----
    df["sma_5"] = df["close"].rolling(5).mean()
    df["sma_10"] = df["close"].rolling(10).mean()
    df["sma_20"] = df["close"].rolling(20).mean()

    # ---- EMA ----
    df["ema_10"] = df["close"].ewm(span=10, adjust=False).mean()
    df["ema_20"] = df["close"].ewm(span=20, adjust=False).mean()

    # ---- RSI (14) ----
    delta = df["close"].diff()
    gain = (delta.where(delta > 0, 0)).ewm(span=14, adjust=False).mean()
    loss = (-delta.where(delta < 0, 0)).ewm(span=14, adjust=False).mean()
    rs = gain / loss
    df["rsi_14"] = 100 - (100 / (1 + rs))

    # ---- ATR (14) ----
    prev_close = df["close"].shift(1)
    tr = np.maximum(df["high"] - df["low"],
                    np.maximum((df["high"] - prev_close).abs(),
                               (df["low"] - prev_close).abs()))
    df["tr"] = tr
    df["atr_14"] = df["tr"].rolling(14).mean()

    # ---- Volatility (20) ----
    df["volatility_20"] = df["log_return"].rolling(20).std()

    # ---- Lag features ----
    df["lag_1"] = df["close"].shift(1)
    df["lag_2"] = df["close"].shift(2)
    df["lag_5"] = df["close"].shift(5)

    # Drop rows with any NaNs generated by rolling/shift
    df = df.dropna().reset_index(drop=True)

    return df


# ---------- DATA LOADING & DATASET BUILDING ----------

def load_stock_df(path: Path) -> pd.DataFrame | None:
    """
    Load one stock CSV from data/allData.
    - Uses ALL rows in file (no time cut-off).
    - Ensures required columns exist.
    - Applies feature engineering.
    Returns df with 'date' + features, or None if unusable.
    """
    try:
        df = pd.read_csv(path)
    except Exception as e:
        print(f"[SKIP] {path.name}: failed to read CSV ({e})")
        return None

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        print(f"[SKIP] {path.name}: missing columns {missing}")
        return None

    try:
        df["date"] = pd.to_datetime(df["date"])
    except Exception as e:
        print(f"[SKIP] {path.name}: date parse error ({e})")
        return None

    df = df.sort_values("date").reset_index(drop=True)
    df = df[["date", "open", "high", "low", "close", "volume"]]

    try:
        df = add_features(df)
    except Exception as e:
        print(f"[SKIP] {path.name}: feature engineering failed ({e})")
        return None

    if len(df) <= SEQ_LEN + 1:
        print(f"[SKIP] {path.name}: not enough rows after features (len={len(df)})")
        return None

    return df


def build_dataset():
    """
    Build X, y from ALL usable stocks in data/allData using full history.
    X shape: (num_samples, SEQ_LEN, num_features)
    y shape: (num_samples,) -> next-day normalized close
    """
    print(f"Scanning {ALLDATA_DIR} for CSV files...")
    all_files = sorted(p for p in ALLDATA_DIR.glob("*.csv"))
    if not all_files:
        raise RuntimeError(f"No CSV files found in {ALLDATA_DIR}")

    stock_data = {}
    all_features_list = []

    for f in all_files:
        symbol = f.stem.upper()
        df = load_stock_df(f)
        if df is None:
            continue

        feature_cols = [c for c in df.columns if c != "date"]
        feats = df[feature_cols].values.astype("float32")

        stock_data[symbol] = (df, feature_cols)
        all_features_list.append(feats)
        print(f"[OK]   {symbol}: using {len(df)} rows")

    if not all_features_list:
        raise RuntimeError("No usable stocks found after filtering / feature engineering.")

    # Global min/max per feature across all stocks
    all_features = np.vstack(all_features_list)
    feat_min = all_features.min(axis=0)
    feat_max = all_features.max(axis=0)
    denom = feat_max - feat_min
    denom[denom == 0] = 1.0

    print("Global feature mins:", feat_min)
    print("Global feature maxs:", feat_max)

    X_list, y_list = [], []

    for symbol, (df, feature_cols) in stock_data.items():
        feats = df[feature_cols].values.astype("float32")
        feats_norm = (feats - feat_min) / denom

        try:
            close_idx = feature_cols.index("close")
        except ValueError:
            print(f"[WARN] {symbol}: 'close' not found in feature columns, skipping")
            continue

        n = len(feats_norm)
        if n <= SEQ_LEN + 1:
            print(f"[SKIP] {symbol}: not enough rows after norm (len={n})")
            continue

        for i in range(n - SEQ_LEN - 1):
            window = feats_norm[i:i + SEQ_LEN]
            target = feats_norm[i + SEQ_LEN, close_idx]
            X_list.append(window)
            y_list.append(target)

    if not X_list:
        raise RuntimeError("No training sequences could be built from available data.")

    X = np.stack(X_list)
    y = np.array(y_list, dtype="float32")

    print(f"Built dataset with features: X shape = {X.shape}, y shape = {y.shape}")
    return X, y, feat_min, feat_max


# ---------- TRANSFORMER MODEL ----------

def transformer_block(x, num_heads, ff_dim, dropout_rate):
    """
    Single Transformer encoder block:
    - Multi-head self-attention
    - Add & Norm
    - Feed-forward (ff_dim -> model_dim)
    - Add & Norm
    """
    model_dim = x.shape[-1]

    attn_output = MultiHeadAttention(
        num_heads=num_heads,
        key_dim=max(model_dim // num_heads, 1),
        dropout=dropout_rate,
    )(x, x)
    x = LayerNormalization(epsilon=1e-6)(x + attn_output)

    ff_output = Dense(ff_dim, activation="relu")(x)
    ff_output = Dropout(dropout_rate)(ff_output)
    ff_output = Dense(model_dim)(ff_output)

    x = LayerNormalization(epsilon=1e-6)(x + ff_output)
    return x


def build_transformer_model(seq_len: int, num_features: int):
    """
    Encoder-style Transformer for time-series regression:
    Input -> Dense(64) -> 2 x Transformer blocks
    -> GlobalAveragePooling1D -> Dense -> Output
    """
    inputs = Input(shape=(seq_len, num_features))

    # Project to model_dim = 64
    x = Dense(64)(inputs)

    x = transformer_block(x, num_heads=4, ff_dim=128, dropout_rate=0.1)
    x = transformer_block(x, num_heads=4, ff_dim=128, dropout_rate=0.1)

    x = GlobalAveragePooling1D()(x)

    x = Dense(64, activation="relu")(x)
    x = Dropout(0.2)(x)
    outputs = Dense(1)(x)

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer="adam", loss="mse")
    model.summary()
    return model


# ---------- TRAINING ----------

def train_model():
    X, y, feat_min, feat_max = build_dataset()
    num_features = X.shape[-1]

    n = len(X)
    split = int(n * 0.8)
    X_train, X_val = X[:split], X[split:]
    y_train, y_val = y[:split], y[split:]

    print(f"Train samples: {len(X_train)}, Val samples: {len(X_val)}")

    model = build_transformer_model(SEQ_LEN, num_features)

    checkpoint_cb = ModelCheckpoint(
        MODEL_PATH.as_posix(),
        monitor="val_loss",
        save_best_only=True,
        verbose=1
    )
    earlystop_cb = EarlyStopping(
        monitor="val_loss",
        patience=6,
        restore_best_weights=True,
        verbose=1
    )

    print("Starting training (Transformer ALL DATA)...")
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        verbose=1,
        callbacks=[checkpoint_cb, earlystop_cb]
    )

    model.save(MODEL_PATH.as_posix())
    print(f"Model saved to {MODEL_PATH}")

    np.savez(SCALER_PATH, feat_min=feat_min, feat_max=feat_max)
    print(f"Scaler saved to {SCALER_PATH}")

    return history


if __name__ == "__main__":
    train_model()
